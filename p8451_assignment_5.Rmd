---
title: "p8451_assignment_5"
output: html_document
date: "2023-02-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 0: Data Cleaning

### Loading packages and preparing dataset

To proceed with the problem set, the following libraries will be used in addition to base R
```{r}
library(tidyverse)
library(dplyr)
library(caret)
library(glmnet)
library(klaR)
library(pscl)

set.seed(123)
```

The data set is first imported using the `read_csv` function, and is cleaned using the `clean_names` function. The data set is then summarised using the `skim` function. The outcome category `alc_consumption` is converted from a character to a factor variable and the ID variable `x1` is then stripped off. 

```{r}
alcohol_data = read_csv(file = "data/alcohol_use.csv") %>%
  janitor::clean_names()
skimr::skim(alcohol_data)

alcohol_data = 
  alcohol_data %>%
  mutate(
    alc_consumption = factor(alc_consumption, labels = c("Current Use", "Not Current Use"))
  ) %>%
  dplyr::select(-x1) %>%
  na.omit()

skimr::skim(alcohol_data)
```

The data set `alcohol_data` now has 1885 observations with 8 variables, all of which are numeric except for the outcome variable `alc_consumption`, which was converted into a factor variable. 

### Creating balanced partitions in the data 

The data is then partitioned into training and testing using a 70/30 split by using the function `createDataPartition`. The training and testing data set is generated with an equal proportion of individuals with the outcome of interest, `alc_consumption`. 

```{r}
train_indices = createDataPartition(y=alcohol_data$alc_consumption, p = 0.7, list = FALSE)

alcohol_train = alcohol_data[train_indices,]
alcohol_test = alcohol_data[-train_indices,]
```

## Part 1: Creating and comparing three different models 

The following three models will be created and compared: 

* Elastic net model - a model that chooses alpha and lambda via cross-validation using all of the features

* Traditional logistic regression - a model that uses all the features and traditional logistic regression 

* Lasso model - a lasso model using all of the features

The value of the alpha parameter dictates whether the model is a ride gression, lasso, or elastic net. A value of 0 is the ridge regression, a value of 1 is a lasso, and any value between 0 and 1 will result in an elastic net model. We will use the caret package to select the best values of alpha and lambda via cross-validation. 

### Elastic net model 

To create the elastic net model, we will use the function `tuneLength` to set the number of combinations of different values of alpha and lambda to compare. In this model, we set tunelength to 10 to result 10 values of alpha and 10 values of lambda. We then print the values of alpha and lambda that give the best prediction (the value in which RMSE is most minimised). 

To obtain the model coefficients at the best tune, we use the function `coef`  
```{r}
set.seed(123)
en.model = train(
  alc_consumption ~., 
  data = alcohol_train, 
  method = "glmnet",
  trControl = trainControl("cv", number = 10), 
  preProc = c("center", "scale"),
  tuneLength = 10
)

en.model$bestTune %>%
  knitr::kable()

coef(en.model$finalModel, en.model$bestTune$lambda) 
```

The values of alpha and lambda that give the best prediction are 0.7 and 0.2578427, respectively. This model demonstrates that impulsiveness_score is the strongest predictor for current alcohol consumption. It negatively predicts `alc_consupmtion`. 

The function `predict` is then used to make predictions in the test set `alcohol_test`, where the finalModel is fed into the test set to predict. We can then obtain estimates of the prediction performance in the test set using `postResample`

```{r}
set.seed(123)
en.pred = en.model %>%
  predict(alcohol_test)

postResample(en.pred, alcohol_test$alc_consumption) %>%
  knitr::kable()
```

The accuracy and kappa values for this elastic net model are 0.8548673 and 0.7028109, respectively.

### Traditional logistic regression model 

To create a logistic regression model, we will use the function `glm` and specify family = `binomial` so taht R fits a logsitic regression model to the training data set. 

```{r}
set.seed(123)
log.model = glm(alc_consumption ~ neurotocism_score +
                  extroversion_score +
                  openness_score + 
                  agreeableness_score +
                  conscientiousness_score + 
                  impulsiveness_score +
                  sens_seeking_score, 
                  family = "binomial",
                  data = alcohol_train)

summary(log.model)
```

The summary of `log.model` illustrates that for every one unit increase in `impulsiveness_score`, there is an average decrease of 1.73910 in the log odds of `alc_consumption`. The p-values of the model indicate that `extroversion_score`, `impulsiveness_score`, and `sens_seeking_score` are predictor variables with significant p-values (p < 0.05). 

To asess the model fit, we can compute the McFadden's R^2, which ranges from 0 to just under 1. Values close to 0 indicate a model with no low predictive power. Values over 0.40 indicate a model that fits the data well. To compute McFadden's R^2 for the logistic regression model, we use the `pR2` function from the pscl package. 

```{r}
pscl::pR2(log.model)
```

The model has a McFadden's R^2 value of 0.333782, indicating the model does not fit the data very well and has a relatively low predictive power. 

### Lasso model

In a lasso model, alpha is set to a value of 1. Cross validation can be used to select the best value of lambda. To do this, a grid first needs to be created to search lambda. The function `tuneGrid` is used in lieu of `tuneLength` to fix the alpha value and to manually select the best lambda value

To obtain the model coefficients at the best tune, we use the function `coef`

```{r}
set.seed(123)
lambda = 10^seq(-3, 3, length = 100)

la.model = train(
  alc_consumption ~., 
  data = alcohol_train, 
  method = "glmnet", 
  trControl = trainControl("cv", number = 10), 
  preProc = c ("center", "scale"),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)

la.model$bestTune %>%
  knitr::kable()

coef(la.model$finalModel, la.model$bestTune$lambda) 
```

The values of alpha and lambda that give the best prediction are 1 and 0.231013, respectively. 

Just as with the elastic net model, the function `predict` is then used to make predictions in the test set `alcohol_test`, where the finalModel is fed into the test set to predict. We can then obtain estimates of the prediction performance in the test set using `postResample`

```{r}
set.seed(123)
la.pred = la.model %>%
  predict(alcohol_test)

postResample(la.pred, alcohol_test$alc_consumption) %>%
  knitr::kable()
```

The accuracy and kappa values for this elastic net model are XXX and XXX, respectively.

Based off of these three models, the model most appropriate for the final model is XXX. Testing the logistic regression model revealed that it had a relatively low predictive power. Comparing the elastic net and lasso model revealed that the XXX model is more appropriate, as it's accuracy value is higher.

## Part 2: Applying final model to the test set

```{r}
control_settings = trainControl(method = 'cv')
```



## Part 3: Real World Application

This analysis could directly address the research question of what personality traits are most associated with high alcohol consumption. It can help identify which individuals are considered at high risk for regularly consuming alcohol, allowing proper health interventions to be made. Alcohol consumption is associated with a wide number of diseases and health issues such as cardiovascular and liver disease. Understanding which individuals are at higher risk for consumption of alcohol can help doctors and public health officials understand who is at risk and what interventions can be proposed before alcohol consumption manifests into larger, more pressing health issues. 

